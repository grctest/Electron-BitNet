{
  "title": "Befehlsmodus",
  "description": "Ein Gespräch mit dem 1-Bit-LLM führen. ",
  "commandOptions": "Befehlsoptionen",
  "numberOfTokens": "Max -Token pro Runde",
  "model": "Modell",
  "threads": "Themen",
  "contextSize": "Kontextgröße",
  "temperature": "Temperatur",
  "prompt": "Prompt",
  "numberOfTokensInfo": "Die maximale Anzahl von Token (Wörtern) Die KI erzeugt in jeder Antwort.",
  "modelInfo": "Wählen Sie das quantisierte '{{fileFormat}}\"Modelldatei mit Microsofts BitNet generiert\"{{script}}'Skript.",
  "threadsInfo": "Die Anzahl der Threads, die zum Ausführen der Inferenz verwendet werden sollen, beschränkt sich auf die Anzahl der auf der CPU verfügbaren Threads.",
  "contextSizeInfo": "Die Größe des schnellen Kontextes bestimmt, wie viel der Gesprächsgeschichte während der Inferenz berücksichtigt wird.",
  "temperatureInfo": "Steuert die Zufälligkeit des generierten Textes. ",
  "promptInfo": "Dies ist der anfängliche Text, mit dem das Modell die Ausgabe generiert.",
  "runInference": "Inferenz laufen",
  "stopInference": "Stoppen Sie den Inferenz",
  "response": "Antwort",
  "license": "{{license}} Lizenzierter Code",
  "builtWith": "gebaut mit",
  "noModelSelected": "Kein Modell ausgewählt",
  "systemPrompt": "Systemaufforderung",
  "systemPromptInfo": "Definieren Sie die Rolle oder Persönlichkeit der KI (z. B. \"Sie sind ein hilfreicher Assistent.\", \"Sie sind ein Pirat\"). ",
  "systemPromptPlaceholder": "Geben Sie hier die Systemaufforderung ein ...",
  "startConversation": "Gespräch beginnen",
  "starting": "Start ...",
  "stopConversation": "Beendigung des Gesprächs",
  "chat": "Chat",
  "typing": "KI tippt ...",
  "typeMessagePlaceholder": "Geben Sie Ihre Nachricht hier ein ...",
  "sendMessage": "Nachricht senden",
  "you": "Du",
  "ai": "Ai",
  "youAvatar": "Du",
  "aiAvatar": "Ai",
  "regenerate": "Antwort neu generieren",
  "copy": "Antwort kopieren",
  "copied": "Kopiert!",
  "expand": "Antwort erweitern",
  "collapse": "Antwort reduzieren"
}